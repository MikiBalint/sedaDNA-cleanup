---
title: "Lab randomization of samples is important"
author: Miki Bálint, Orsolya Márton, Marlene Schatz, Rolf-Alexander Düring, Hans-Peter
  Grossart
output:
  pdf_document: default
  html_document: default
---

# Load libraries
``` {r}
library(vegan)
library(effects)
library(lme4)
library(mvabund)
```

```{r eval=FALSE, include=FALSE}
# Other possible packages

# Color palette
# palette(colors())
# library(knitr)
# library(boral)
# library(corrplot)
# library(leaps)
# library(car)
# library(mgcv)
# library(geosphere)
# library(ape) # installed with ctv, infos here: http://www.phytools.org/eqg/Exercise_3.2/
# library(vegan3d)
# library(bvenn)
```

# Read in data
```{r}
# Read abundances
EmblAssign = read.csv(file="../../Data/stechlin_assigned_190915.tab",
                      header=T, sep='|', row.names = 1)

# Read POP and elements data
POP_elem = read.csv(file = "../../Data/stechlin_pop_elements.csv", 
                    header = T, row.names = 1)

# Read experimental setup data
ExpSet = read.csv(file = "../../Data/sample_infos.csv",
                  header = T, row.names = 1)
RepliExp = ExpSet[1:96,]

# re-order the horizon identities
RepliExp$depth.nominal = factor(RepliExp$depth.nominal)

# re-order the nuclear power plant periods
RepliExp$nuclear = factor(RepliExp$nuclear,
                          levels=c("before", 
                                   "during", "after"))
```

How many reads are there?
```{r}
IndexSample = grep("sample.[EMPS]", names(EmblAssign))
sum(EmblAssign[,IndexSample])
```

# Data cleanup
The aim is double: 1) to deal with potential contaminants that may show up in the negative, and 2) to establish a read abundance credibility treshold using the samples that were in the positive controls. 

The assumptions of 1) is that contaminant OTUs observerd in the negatives had their best chances to be present in large numbers only in those negatives since they would have faced PCR competition in any other samples. Thus the most conservative way of dealing with contaminant OTUs is to subtract the maximum number of reads that belongs to them from any other sample where they were also observed.

The reasons behind 2) is that there were only known taxa put into the positives and if others are showing up, they may only come from rare errors. Thus unexpected taxon reads in the positives should give a general sequence rarity treshold for the entire dataset.

### Heads, singletons, internals
This step is specific to the OBITools sequence processing pipeline.

Which columns have the status info for head, internal, singletons?
```{r}
StatusEmbl = EmblAssign[,grepl("obiclean.status", names(EmblAssign))]
```

Keep only sequence variants that were seen at least once as 'head' or 
the 'singleton' count is higher than the 'intermediate' count.

```{r}
EmblHead = EmblAssign[(EmblAssign$obiclean_headcount) > 0 | 
                        EmblAssign$obiclean_singletoncount > EmblAssign$obiclean_internalcount,]
```

### Clean up negative controls
Remove the maximum read number of a sequence variant 
found in a negative control from every sample that 
contains that variant

Select the extraction controls
```{r}
# Extraction controls
ExtCont = grep("sample.EXT", names(EmblHead))

# PCR controls
PCRCont = grep("sample.PCR", names(EmblHead))

# Multiplexing controls
MPXCont = grep("sample.MPX", names(EmblHead))
```

Write out negative control assignments for more analysis
```{r}
NegCont = cbind(name = EmblHead$scientific_name, 
                EmblHead[,c(ExtCont, PCRCont, MPXCont)])

# Aggregate according to taxon
NegRegate = aggregate(. ~ name, NegCont, sum, na.action = na.exclude)
rownames(NegRegate) = NegRegate$name
NegRegate = NegRegate[,2:length(names(NegRegate))]

# Remove taxa that were not seen in negatives
NegRegate = NegRegate[apply(NegRegate,1,sum) > 0,]

# Write for checking negatives
write.csv(file = "negative_control_identities.csv", NegRegate)
```

How many reads are there before cleaning up the negatives?
```{r}
sum(EmblHead[,IndexSample])
```

`sweep` the maximum reads of a sequence variant
in any controls from all samples
```{r warning=FALSE}
# warnings supressed because of a numeric-non-numeric substitution,
# see below
MaxControl = apply(EmblHead[,c(ExtCont,PCRCont,MPXCont)], 1, max)

EmblControlled = EmblHead

# sweep with the MaxControl
EmblControlled[,grep("sample", names(EmblHead))] <- 
  sweep(EmblHead[,grep("sample", names(EmblHead))], 1, MaxControl, "-")

# Set negative values to 0. Warnings are because the non-numeric cells
# There are warnings since many fields are non-numeric.
EmblControlled[EmblControlled < 0] <- 0

# Remove sequence variants with no reads left
EmblControlled = EmblControlled[apply(EmblControlled[,grep("sample", names(EmblHead))],1,sum) > 0,]
```

How many reads are there after cleaning up the negatives?
```{r}
sum(EmblControlled[,IndexSample])
```

Write sequence variants of taxa into table
```{r}
write.csv(file = "variant_sequences.csv", 
          data.frame(name = rownames(EmblControlled),                   
                     name = EmblControlled$scientific_name,
                     genus = EmblControlled$genus_name,
                     family = EmblControlled$family_name,
                     seq = EmblControlled$sequence))
```

### Rare read cleanup with positives controls
Taxa in positive controls

```{r}
PosCont = grep("^sample.POS", names(EmblControlled))

# Abundances in positives and taxonomic annotations
PosAbundances = EmblControlled[,c(PosCont,409,10,9)]

# Sums of all reads / OTU and add to positive table
SumReads = apply(EmblControlled[,IndexSample], 1, sum)
PosAbundances = cbind(PosAbundances, SumReads)

# keep only OTUs that had reads
PosAbundances = PosAbundances[apply(PosAbundances[,1:2],1,sum) > 0,]
```

Write out positive control samples + 
scientific name + genus + family name.
```{r}
write.csv(file="positive_controls.csv", PosAbundances)
```

I manually compared the reads of these OTUs to the 
taxon list and DNA concentration list of the positives.
I looked for a read count rarity threshold of the complete read numbers
`942742` that would reflect the original positive taxon lists.

When an OTU is present with less, than `14` reads in a replicate 
(`0.0015%` of the total reads `sum(ExpSet$reads)` 942742, is considered erroneous and set to `zero`.

```{r warning=FALSE}
RareControlled <- EmblControlled
RareControlled[RareControlled < 14] <- 0
```

How many reads after removing rare observations?
```{r}
sum(RareControlled[,IndexSample])
```


# Effects of laboratory biases
I use all replicates here however weird (similar to negatives or positives, wide away from the other replicates form the same horizon). 

The reason - the aim here is to quantify the effects of potentially systemic lab biases, so all biases qualify except standard contamination issues and rare weird sequences.

## Generate the needed matrices: OTU abundances and corresponding predictors.

```{r}
# Predictors of lab effects
ExpLab = RepliExp[13:96,]

# OTU abundance matrix
OTULab = RareControlled[,IndexSample]
OTULab = data.frame(t(OTULab[,13:96]))

# remove OTUs with zero reads
OTULab = OTULab[,apply(OTULab,2,sum) > 0]

# there is a sample with no reads left, it is removes
OTULab = OTULab[apply(OTULab,1,sum) > 0,]

# Adjust the explaining vars
ExpFilter = rownames(ExpLab) %in% rownames(OTULab)

ExpLab = ExpLab[ExpFilter,]
```

The possible systemic bias to be tested is the person who performed the DNA extraction. The extractions were initially planned to be performed by the first author of the paper, but finally the second half of the extractions were performed by the second author due to an unexpected urgency. Visually it seems that the extractions done by the second author generally yielded more DNA.
```{r}
boxplot(ExpLab$conc ~ ExpLab$person,
     ylab="Extracted DNA concentration (ng/ul)",
     main = "", pch = 19, col="grey", boxwex=0.5, notch = T)
```

Linear mixed effect models are used for the singe response models with the sediment profile identity as the random effect.

## The variables of interest and their predictors 
1. DNA concentration
  + Expected laboratory effect: 
    + sample weight
    + extraction kit
  + Unexpected laboratory effect:
    +lab person
  + Biological effect of interest:
    + the age of the sediment with a higher hump at intermediate ages (visible from an exploratory plot)
    
```{r}
plot(ExpLab$conc ~ ExpLab$age, pch=19,
     xlab = "Age (years)", ylab = "DNA concentration (ng/ul)")
```
    
2. PCR success as indicated by the number of sequence reads per sample. PCR product concentrations were not normalized before multiplexing the samples for sequencing, thus the differences in read numbers may be used to evaluate PCR performance (e.g. `Taberlet papers`)
  + Expected laboratory effect:
    + DNA concentration
    + extraction kit
  + Unexpected laboratory effect:
    + lab person
  + biological effect of interest:
    + sediment age
3. Community structure: diversity indicators (Hill's 1st, 2nd and 3rd numbers) and community composition
  + Expected laboratory effect:
    + read numbers
    + extraction kit
  + Unexpected laboratory effect:
    + lab person
  + biological effect of interest:
    + the effects of the construction/operation period of a nuclear power plant (1960 - 1990). We know that lake communities were strongly changed after building the plant from previous morphology-based works.

## DNA concentration
Fit the full model
```{r}
conc.weight.kit.person.age = 
  lmer(conc ~ weight + kit +
       person + age + I(age^2) + 
         (1|depth.nominal),
     data = ExpLab)

conc.weight.kit.age = 
  lmer(conc ~ weight + kit +
       age + I(age^2) + 
         (1|depth.nominal),
     data = ExpLab)
```

The DNA concentration model is marginally statistically significantly improved by accounting for the lab person.

```{r}
anova(conc.weight.kit.person.age,
      conc.weight.kit.age)
```

Visualize the variable effects. For some reason the `effects` package does not plot into the knitted document, so I use a saved image.
```{r eval=FALSE}
plot(allEffects(conc.weight.kit.person.age,
                multiline=TRUE, confidence.level = 0.95))
```
![Predictors of extracted DNA concentration](conc_effect.png)

Most of the variation in the DNA concentration is explained by the isolation kit. The lab person explains about as much variation as the age of the sediment.
```{r}
anova(conc.weight.kit.person.age)
```

## PCR success
```{r}
read.conc.kit.pers.age = 
  lmer(reads ~ conc + kit + person + age + (1|depth.nominal),
     data = ExpLab)

read.conc.kit.age =
  lmer(reads ~ conc + kit + age + (1|depth.nominal),
       data = ExpLab)
```

The model of PCR success was statistically significantly improved by considering the lab person.
```{r}
anova(read.conc.kit.pers.age,
      read.conc.kit.age)
```

The **DNA concentration** had no consistent effects on the PCR success within a 95% confidence interval (although PCRs produced more reads with higher template concentrations). The **age** of the sediment had no effect on PCR success.

```{r eval=FALSE}
plot(allEffects(read.conc.kit.pers.age,
                multiline=TRUE, confidence.level = 0.95))
```
![Predictors of the obtained read numbers per sample (PCR success)](read_effect.png)

The lab person explained far the most variation in PCR success, with the samples extracted by the second authors yielding consistently less reads (all PCR reactions were performed by the second author). 
```{r}
anova(read.conc.kit.pers.age)
```

# Community structure
## Diversity indicators

Hill's first three numbers of diversity corrspond to richness (Hill's 1), the exponent of Shannon diversity (Hill's 2) and the inverse of the Simpson diversity (Hill's 3).
```{r}
HillLab = renyi(OTULab, scale = c(0,1,2), hill = T)
names(HillLab) = c("hill1", "hill2", "hill3")
```

### Hill's 1 (richness)

```{r}
hill1.read.kit.pers.nucl = 
  lmer(HillLab$hill1 ~ reads + person + kit + nuclear +
         (1|depth.nominal),
     data = ExpLab)

hill1.read.kit.nucl = 
  lmer(HillLab$hill1 ~ reads + kit + nuclear +
         (1|depth.nominal),
     data = ExpLab)
```

Considering the lab person did not 
```{r}
anova(hill1.read.kit.pers.nucl,
      hill1.read.kit.nucl)
```

The richness model was not improved by including the lab person as predictor.

```{r include = FALSE}
plot(allEffects(hill1.read.kit.pers.nucl,
                multiline=TRUE, confidence.level = 0.95))
```
![Predictors of Hill's 1 (richness)](hill1_effect.png)

Most variation in richness was explained by the PCR success, followed by the operation period of the nuclear power plant. The extraction kits also consistently predicted variation in richness, with more OTUs recorded in the replicates extracted with the Macherey-Nagel kit. The variation explained by the lab person was minor. Communities in the lake were less species-rich following the construction of the plant.

```{r}
anova(hill1.read.kit.pers.nucl)
```

### Hill's 2 (exp(Shannon diversity))

```{r}
hill2.read.kit.pers.nucl = 
  lmer(HillLab$hill2 ~ reads + kit + person + nuclear +
         (1|depth.nominal),
     data = ExpLab)

hill2.read.kit.nucl = 
  lmer(HillLab$hill2 ~ reads + kit + nuclear +
         (1|depth.nominal),
     data = ExpLab)
```

The model of Hill's 2 was not improved by considering the lab person's identity.
```{r}
anova(hill2.read.kit.pers.nucl,
      hill2.read.kit.nucl)
```

Hill's 2 was strongly influenced by the PCR success. The replicates extracted with the MN-kit consistently had higher Hill's 2 values. Hill's 2 strongly decreased after the construction of the power plant.

```{r include = FALSE}
plot(allEffects(hill2.read.kit.pers.nucl,
                multiline=TRUE, confidence.level = 0.95))
```
![Predictors of Hill's 2](hill2_effect.png)

The identity of the lab person explains almost no variation in Hill's 2.
```{r}
anova(hill2.read.kit.pers.nucl)
```

### Hill's 3 (inverse Simpson)
Fit the full model.
```{r}
hill3.read.kit.pers.nucl = 
  lmer(HillLab$hill3 ~ reads + kit + person + nuclear +
         (1|depth.nominal),
     data = ExpLab)

hill3.read.kit.nucl = 
  lmer(HillLab$hill3 ~ reads + kit + nuclear +
         (1|depth.nominal),
     data = ExpLab)
```

The lab person identity did not improve the Hill's 2 model.
```{r}
anova(hill3.read.kit.pers.nucl,
      hill3.read.kit.nucl)
```

The effects of the nuclear power plant explained most variation in Hill's 3, resulting in lower diversity after. These were followed by the PCR success and the effects.
```{r include = FALSE}
plot(allEffects(hill3.read.kit.pers.nucl,
                multiline=TRUE, confidence.level = 0.95))
```
![Precictors of Hill's 3](hill3_effect.png)

The lab person explained little variation in Hill's 3.
```{r}
anova(hill3.read.kit.pers.nucl)
```

## Community composition
### Quick NMDS
```{r}
MDS.all <- metaMDS(OTULab)
MDS.all
```

Plot and color the ordination
```{r}
# colors
colfunc <- colorRampPalette(c("green", "brown"))
MyColors = colfunc(83)

par(mar=c(4,4,1,1))
plot(MDS.all$points, type="n",
     xlab="Community axis 1", ylab="Community axis 2")
ordispider(MDS.all, ExpLab$depth.nominal, col="grey")
# plot the points re-ordered according to depth
points(MDS.all$points[order(ExpLab$depth),], pch=19, col = MyColors)
ordisurf(MDS.all, ExpLab$age, add=T, col =
           "grey")
ordiellipse(MDS.all, ExpLab$nuclear,cex=.5, 
            draw="polygon", col=c("green"),
            alpha=100,kind="se",conf=0.95, 
            show.groups=(c("before")))
ordiellipse(MDS.all, ExpLab$nuclear,cex=.5, 
            draw="polygon", col=c("orange"),
            alpha=100,kind="se",conf=0.95, 
            show.groups=(c("during")))
ordiellipse(MDS.all, ExpLab$nuclear,cex=.5, 
            draw="polygon", col=c("blue"),
            alpha=100,kind="se",conf=0.95, 
            show.groups=(c("after")))
# # mylegend = legend(-1, 0.95, c("North, natural","North, heated","South"), 
# ordisurf(MDS.all, ExpLab$DDE.4.4, add=T, col =
#            "red")
```
The replicates are colored according to the sample. Note the weird replicates those all pull toward the negative - positive controls on the complete ordination (not yet shown). The ellipses show the 95% CI group centroids of replicates from before 1960 (green), between 1960-1990 (orange), after 1990 (blue). The countours mar the sediment age. 

### Models of community
I need a general, community-level statistic. `mvabund` cannot deal with random effects: I ommit the horizon identity completely and treat the replicates as independent although they are not

I will compare this with `PERMANOVA` in the `adonis` with `strata` specified as horizons to consider nestedness.

Fit the community model
```{r}
# input data
OTU.mva = mvabund(OTULab)

# full model fit
OTU.read.kit.pers.nucl =
  manyglm(OTU.mva ~ reads + kit + person + nuclear,
          data = ExpLab, family = "negative.binomial")
```

Variation partitioning and predictor stat. significance. Save the ANOVA results.
```{r eval=FALSE}
OTU.anova = anova(OTU.read.kit.pers.nucl,
                  nBoot = 100, test = "LR")

save(OTU.anova, file="lab-methods_OTU_anova.RData")
```

Load the saved ANOVA results
```{r}
load("lab-methods_OTU_anova.RData")
```

Variations explained and statistical significances. Most variation is explained by the biological effect, followed by the person (although both are only marginally significant). The kit doesn't seem to influence the community composition.
```{r}
OTU.anova
```

### Ordination with latent variables
Are there OTUs with weird overdispersion parameters? These are calculated by `mvabund`.

```{r}
hist(OTU.read.kit.pers.nucl$theta, 
     col="grey", xlab="Overdispersion parameters of OTUs", nclass=20)
```

Set the dispersion prior (the last of hypparams) according to the range of **theta** from the `mvabund` run.
```{r}
set.prior = list(type = c("normal","normal","normal","uniform"),
                 hypparams = c(100, 20, 100, 30))
```

Fit the `boral` model only with the OTUs that are not exceedingly overdispersed. Run on the `malloy` since it takes a long time.
```{r eval=FALSE}
# LV ordination done on all OTUs with relatively low overdispersion
save.image(file="Lab_Boral_Data.RData")

LabLVMOrd = boral(OTULab[,OTU.read.kit.pers.nucl$theta < 31],
                  X = ExpLab$reads,
                  family = "negative.binomial", 
                  prior.control = set.prior, 
                  num.lv = 2, n.burnin = 1000, 
                  n.iteration = 10000, n.thin = 9)

save(LabLVMOrd, file="Lab_LV_model_10000-iter.RData")
```



# Ecological evaluation of the 11-cm Stechlin core
### Remove weird replicates
Visually selected from the `boral` plot above
```{r eval=FALSE}
# Abundance matrix without the controls
IndexHorizon = grep("sample.ST", names(RareControlled))

# A filter for weird horizon replicates
RepliProblemFilter = 
  names(RareControlled)[IndexHorizon] %in% RepliProblems

# Only the samples kept
HorizonMatrix = RareControlled[, IndexHorizon]

# Weird replicates removed
NoWeirdMatrix = HorizonMatrix[, !(RepliProblemFilter)]
```

### False positive control with detections in replicates
Remove OTUs if not observed in at least 2 out of 4 replicates of a horizon
How to do this was found here: http://stackoverflow.com/questions/9704213/r-remove-part-of-string

Get the sample names. 
```{r eval=FALSE}
SampleNames = levels(as.factor(sapply(strsplit(names(NoWeirdMatrix),
                                               split='ep', fixed=TRUE),
                                      function(x) (x[1]))))
```

In how many replicates observed per sample?
```{r eval=FALSE}
PresentReps = data.frame(row.names = rownames(NoWeirdMatrix))
for (i in SampleNames){
  ActualSet = grep(i, names(NoWeirdMatrix))
  Selected = NoWeirdMatrix[ActualSet]
  Selected[Selected > 0] <- 1 # set the read numbers to 1
  PresentReps = cbind(PresentReps, apply(Selected, 1, sum))
}
colnames(PresentReps) = SampleNames
```

Set read numbers to 0 in a sample if the sequence variant was not observed in at least two PCR replicates
```{r eval=FALSE}
AveragedReps = data.frame(row.names = rownames(NoWeirdMatrix))
for (i in SampleNames){
  ActualSet = grep(i, names(NoWeirdMatrix)) # grep the columns of interest
  AveragedReps = cbind(AveragedReps, 
                       apply(NoWeirdMatrix[ActualSet], 1, mean))
}
colnames(AveragedReps) = SampleNames

# Remove OTU observations if not seen in at least two replicates
AveragedReps[PresentReps < 2] <- 0

# Remove OTUs with no reads left
AveragedReps = AveragedReps[apply(AveragedReps,1,sum) > 0,]

# Round up the reads
AveragedReps = ceiling(AveragedReps)
```

How many reads were found in the horizons? These are already averaged.
```{r eval=FALSE}
sum(AveragedReps)
```

Read abundances, taxonomy and sequences of the remaining OTUs
```{r eval=FALSE}
# Filter the remaining "clean" OTUs
FilterOTUs = rownames(EmblAssign) %in% rownames(AveragedReps)

# write out the final annotated OTU list
write.csv(file = "OTU_abundance_table.csv",
          cbind(AveragedReps, 
                scientific_name = EmblAssign$scientific_name[FilterOTUs],
                genus_name = EmblAssign$genus_name[FilterOTUs],
                family_name = EmblAssign$family_name[FilterOTUs],
                sequence = EmblAssign$sequence[FilterOTUs]))
```

```{r eval=FALSE,include=FALSE}
########
# Community analyses
# Variable relationships
# Histogram of metals and pesticides
pdf(file="POP_elem_histo.pdf", height = 10, paper = "a4")
par(mfrow=c(6,4))
for (i in names(POP_elem[1:21,c(3:8,14:31)])) {
  hist(POP_elem[,i], main = i)
}
dev.off()

# Element correlations
pdf(file="POP_elem_correlations.pdf")
corrplot(cor(na.omit(POP_elem[1:21,c(2,4:9,15:32)]), 
             method = "spearman"),
         diag = F, 
         order = "hclust", hclust.method = "average")
dev.off()

# POP, elements and depth
pdf(file="POP_elem_depth.pdf", height = 10, paper = "a4r")
par(mfrow=c(6,4), mar = c(3,3,1,1))
for (i in names(POP_elem[,c(4:9,15:32)])) {
  plot(POP_elem$depth[1:21], POP_elem[1:21,i], pch = 19, cex = 0.7,
       main = i, xlab = "depth", ylab = "", type = "o")
  abline(v=2.9, col="red")
  abline(v=6.7, col="red")
}
dev.off()

# Clustering of variables
pdf(file="POP_elem_cluster.pdf")
plot(hclust(dist(cor(na.omit(POP_elem[1:21,c(2,4:9,15:32)])))),
     xlab = "")
dev.off()

# # PCA of these variables
# POP_elem_PCA = rda(na.omit(POP_elem[1:21,c(3:8,14:31)]))
# 
# # PC correlations with variables
# corrplot(cor(data.frame(POP_elem_PCA$CA$u[,1:4],
#                         na.omit(POP_elem[1:21,c(3:8,14:31)])),
#              method = "spearman"),
#          diag = F,
#          order = "hclust", hclust.method = "average")

# Centering and co-plotting of pesticides, metals and OTUs

##### Pesticide, elements and eDNA visualizations
# depths corresponding the samples
depths = read.csv(file="depths.csv", header=T)

# Transposed species abundance matrix
SamCountsT = t(SamCounts)

# Z-scores of counts
SamZ = scale(OTU.some, center = T, scale = T)

# Center the POP and element data to get z-scores
PE_scaled = cbind(POP_elem[,1:2], 
                  scale(POP_elem[,c(4:7, 15:length(names(POP_elem)))], 
                        center = T, scale = T))

# Plot together with elements and POPs
par(mfrow = c(1,3))
# POP
par(mar = c(2,3,2,0))
plot(seq(min(PE_scaled[,3:7], na.rm=T), 
         max(PE_scaled[,3:7], na.rm=T), 
         (max(PE_scaled[,3:7], na.rm=T) - 
            min(PE_scaled[,3:7], na.rm=T))/(nrow(PE_scaled)-1)),
     PE_scaled$depth, 
     type = "n", cex.axis = 1.3,
     ylim = rev(range(PE_scaled$depth)),
     main = "Pesticides", xlab = "Depth (cm)")
for (i in names(PE_scaled[3:7])){
  points(PE_scaled[,i], PE_scaled$depth, 
         type = "l", col = sample(90:99), lwd=2)
}

# Elements
par(mar = c(2,1,2,1))
plot(seq(min(PE_scaled[,8:ncol(PE_scaled)], na.rm=T), 
         max(PE_scaled[,8:ncol(PE_scaled)], na.rm=T), 
         (max(PE_scaled[,8:ncol(PE_scaled)], na.rm=T) - 
            min(PE_scaled[,8:ncol(PE_scaled)], na.rm=T))/(nrow(PE_scaled)-1)), 
     PE_scaled$depth, 
     type = "n", yaxt = "n", cex.axis = 1.3,
     ylim = rev(range(PE_scaled$depth)),
     main = "Metals", xlab = "Depth (cm)")
for (i in names(PE_scaled[8:ncol(PE_scaled)])){
  points(PE_scaled[,i], PE_scaled$depth, 
         type = "l", col = sample(150:159), lwd=2)
}

# Taxa
par(mar = c(2,0,2,3))
plot(seq(min(SamZ, na.rm=T), # keeping PE_scaled$depth: to use the same depth range 
         max(SamZ, na.rm=T), 
         (max(SamZ, na.rm=T) - 
            min(SamZ, na.rm=T))/(nrow(PE_scaled)-1)), 
     PE_scaled$depth,
     ylim = rev(range(PE_scaled$depth)),
     type = "n", yaxt = "n", cex.axis = 1.3,
     main = "Taxa", xlab = "Depth (cm)")
for (i in names(as.data.frame(SamZ))) {
  points(as.data.frame(SamZ)[,i], depths$depth, type = "p", 
         cex=0.8, col = sample(20:29), lwd=2)
}
axis(4, at = pretty(PE_scaled$depth, 5), 
     labels = pretty(rev(PE_scaled$depth), 5), 
     cex.axis = 1.3)

# corrplot(cor(cbind(depth = depths$depth, SamCountsT), method="spearman"),
         # tl.cex = 0.5, tl.col = "black")

# Multiply by 4 do get read numbers of four technical replicates
# and round the floats
SummedMatrix = round(AveragedReps*4)

# Remove zero-count OTUs
SummedMatrix = SummedMatrix[apply(SummedMatrix,1,sum) > 0,]
# sum(SummedMatrix)
# [1] 530604

# rename SummedMatrix column names
names(SummedMatrix) <- gsub("sample.ST01.","ST01_",names(SummedMatrix))

# Summarize all predictors for the 21 horizons
# 0. keep only relevant POP_elem entries
POP_short = POP_elem[1:21,]

# 1. ordering the POP_short names will give the same order 
# that matches names SummedMatrix.
POP_shortReord = POP_short[order(rownames(POP_short)),]

# 2. the "depth" from the POP_elem will re-sort the order in SummedMatrix
names(SummedMatrix)[order(POP_shortReord$depth)]
SummedMatrixReord = SummedMatrix[,order(POP_shortReord$depth)]

# 3. the RepliExp replicates need to be summarized
# This is still ugly.
RepliProblems2 = c("ST01-10-rep-1-MN_S1",
                   "ST01-7-5-rep-1-MB_S90",
                   "ST01-9-rep-1-MB_S14",
                   "ST01-5-5-rep-1-MN_S27",
                   "ST01-3-5-rep-1-MB_S12",
                   "ST01-5-5-rep-1-MB_S66",
                   "ST01-3-rep-1-MB_S40",
                   "ST01-2-rep-1-MB_S62",
                   "ST01-1-rep-2-MN_S51",
                   "ST01-8-5-rep-1-MB_S76")

# remove the controls
ToSumExp = RepliExp[13:96,]

# remove problematic replicates
ToSumExp = ToSumExp[grep(paste(RepliProblems2, 
                               collapse = "|"), 
                         rownames(ToSumExp), invert=T),]

# write out what needs to be combined
write.csv(file = "sum_explanatories.csv", ToSumExp)

# read in combined explanatories
SummedExp = read.csv(file = "averaged_explanatories.csv", header = T,
                     row.names = 1)

# Correct rownames
rownames(SummedExp) = as.factor(sapply(strsplit(rownames(SummedExp),
                                                split='-rep', fixed=TRUE),
                                       function(x) (x[1])))

# Multiply by 4 to get original (or estimated) read numbers 
# from all replicates
SummedExp$conc <- SummedExp$conc*4
SummedExp$reads <- SummedExp$reads*4

# # Sample names for experimental variables
# SampleNamesExp = levels(as.factor(sapply(strsplit(rownames(ToSumExp),
#                                                split='-rep', fixed=TRUE),
#                                       function(x) (x[1]))))
# 
# # Create the dataframe
# SummedExp = data.frame(row.names = SampleNamesExp)
# 
# # averaged reads
# for (i in 1:length(SummedExp)){
#   ActualSet = grep(SampleNamesExp[i], rownames(ToSumExp), fixed=T) # grep the columns of interest
#   AveragedReps = cbind(SummedExp, reads = apply(ToSumExp[ActualSet], 1, mean))
# }
# colnames(AveragedReps) = SampleNames
# grep(SampleNamesExp[3], rownames(ToSumExp), fixed=T)

# 4. Reorder Explanatory matrix
SummedExpReord = SummedExp[order(SummedExp$depth),]

# 4. Are all entries in all dataframes ordered the same way?
data.frame(POP = rownames(POP_short),
           Exp = rownames(SummedExpReord),
           OTU = rownames(t(SummedMatrixReord)))

# 5. correlation structures in POP_short
# pdf(file="POP_elem_correlations.pdf")
corrplot(cor(na.omit(POP_short[,c(2:8,14:31)]), 
             method = "pearson"),
         diag = F, 
         order = "hclust", hclust.method = "average")
# dev.off()

# Most POPs and metals are correlated. 
# DDT has very few data, I leave it out.
# relatively not correlated with others: TF, Mo, Na, 
# but they are hard to interpret
# Finally, I will use depth alone as substitute for all others

# NOT DONE: I will use the POP/metal with the highest correlation coefficient
# for the autocorrelated group

# Are they correctly selected? 
corrplot(cor(na.omit(POP_short[,c(3:6,15:27,30:31)]),
             method = "pearson"),
         diag = F, 
         order = "hclust", hclust.method = "average")

# cumulative correlation coefficients
cor_POP_elem = cor(na.omit(POP_short[,c(2:6,15:27,30:31)]), 
                   method = "spearman")

# substract the diagonal and divide into two 
# because each value is doubled
max((apply(abs(cor_POP_elem),1,sum)-1)/2) # As has the highest cum cor coef

# OTUs in at least two horizons
TotPresent = apply(t(SummedMatrixReord),2,function(vec) sum(vec>0))
OTU.some = SummedMatrixReord[TotPresent > 2,]
OTU.some = as.data.frame(t(OTU.some))

# Models of community composition
OTUSummed.mva = mvabund(OTU.some)

# full model. TF is weird. Mo and Na is hard to interpret. 
OTU.m1 = manyglm(OTUSummed.mva ~ 
                   SummedExpReord$reads +
                   SummedExpReord$person +
                   SummedExpReord$depth,
                 family = "negative.binomial")

# diagnostic plot
plot(OTU.m1, which = c(1:3))

# Variation partitioning
anova.OTU.m1 = anova(OTU.m1, nBoot = 100, p.uni = "adjusted")

# With AKW times
# Add AKW operation to variables
POP_short = data.frame(POP_short, 
                       akw = c(rep("post",4), c(rep("akw",10)), c(rep("pre",7))))

POP_short = data.frame(POP_short, 
                       akw2 = c(rep("NoAkw",4), c(rep("Akw",10)), c(rep("NoAkw",7))))

OTU.m3 = manyglm(OTUSummed.mva ~ depth + akw,
                 data=POP_short,
                 family = "negative.binomial")

OTU.m4 = manyglm(OTUSummed.mva ~ depth + akw2,
                 data=POP_short,
                 family = "negative.binomial")

OTU.m3$AICsum
OTU.m4$AICsum

OTU.m5 = manyglm(OTUSummed.mva ~ depth*akw,
                 data=POP_short,
                 family = "negative.binomial")

OTU.m5$AICsum

anova.OTU.m5 = anova(OTU.m5, nBoot = 50)
anova.OTU.m5$table

summary.OTU.m5 = summary(OTU.m5, nBoot = 50)

# How OTUs change?
# OTUs stat. significantly explained by depth
p.ind.anova <- as.data.frame(anova.OTU.m1$uni.p)
colnames(p.ind.anova)[p.ind.anova[4,]<=0.5]

# OTUs with abs(coef) - abs(stderr) > 0 (reliable abundance change)
# Depth
StrongCoefDepth = abs(OTU.m1$coefficients["SummedExpReord$depth",]) - 
  abs(OTU.m1$stderr.coefficients["SummedExpReord$depth",]) > 0

NamesDepth = names(StrongCoefDepth[StrongCoefDepth == T])

# plot(fitted(OTU.m1)[,"M01271.3.000000000.D0YHF.1.1101.19713.24293_CONS_SUB"],
#      SummedExpReord$depth)
plot(OTU.some$`M01271:3:000000000-D0YHF:1:1101:13412:13277_CONS_SUB`,
    SummedExpReord$depth)

# LVM of horizons
# simple model, similar to the LVM ordination
OTU.m0 = manyglm(OTUSummed.mva ~ 
                   SummedExpReord$reads,
                 family = "negative.binomial")

# diagnostics
plot(OTU.m0, which = c(1:3))

# distribution of the overdispersion parameters
hist(OTU.m0$theta)
hist(log(OTU.m0$theta)) # a few OTUs have weird dispersion parameters

# OTUs with weird overdispersion
names(OTU.some[,OTU.m0$theta > 30])

# LVM model
# Set overdispersion prior
set.prior = list(type = c("normal","normal","normal","uniform"),
                 hypparams = c(100, 20, 100, 20))

# LVM ordination of horizons
horizon.comm.ord = boral(OTU.some[,OTU.m0$theta < 30],
                         X = SummedExpReord$reads,
                         family = "negative.binomial", 
                         prior.control = set.prior, num.lv = 2, n.burnin = 10, 
                         n.iteration = 100, n.thin = 1)

# gradient colors
colorfunk = colorRampPalette(c("green", "brown"))

# try colors
plot(c(1:21), c(1:21), lwd=3, 
     col=colorfunk(21), pch = SummedExpReord$depth_nominal+1)



# depth_names = unique(factor(ExpPredictor$depth))
# pdf(file = "LVM_technical_replicates.pdf", width = 10)
par(mfrow = c(1,1), mar = c(4,4.5,1,1))
hor.ordicomm = ordiplot(horizon.comm.ord$lv.median, choices = c(1,2), type = "none",
                    display = "sites",
                    cex.axis = 1.3, cex.lab = 1.3)
points(hor.ordicomm,"sites", lwd=4, 
       col=colorfunk(21), 
       pch = SummedExpReord$depth_nominal+1,
       cex = 1.5)
legend(-0.05, 0.08, SummedExpReord$depth, border="white", 
       col=colorfunk(21), 
       pch = SummedExpReord$depth_nominal+1, 
       cex = 1, bty="n", lwd = 2, ncol = 2)
ordisurf(hor.ordicomm, POP_short$DDE4.4, 
         add=T, col = "red", lwd = 1.5, cex = 1.3)
ordisurf(hor.ordicomm, POP_short$depth, 
         add=T, col = "black", lwd = 1.5, cex = 1.3)
# ordisurf(hor.ordicomm, POP_short$As.193.696, 
#          add=T, col = "green", lwd = 2, cex = 1.3)
# ordisurf(hor.ordicomm, POP_short$Pb.217.000, 
#          add=T, col = "yellow", lwd = 2, cex = 1.3)
# ordisurf(hor.ordicomm, POP_short$Ca.220.861, 
#          add=T, col = "blue", lwd = 2, cex = 1.3)
# ordisurf(hor.ordicomm, POP_short$Mo.201.512, 
#          add=T, col = "green", lwd = 2, cex = 1.3)
# dev.off()

# SOM2: species modelled separately - for species-specific methodological differences
# Detection probabilities for each species per method
SOM2.p = read.csv(file = "SOM/SOM2_detection_prob.csv", row.names = 1, header=T)

# False positives, each species per method
SOM2.fp = SOM2.p = read.csv(file = "SOM/SOM2_false_pos.csv", row.names = 1, header=T)

# Coefficient plots
par(mfrow=c(1,3), mar=c(4.1,1,0.5,0.5))
plot(rep(0,nrow(SOM2.p)), c(nrow(SOM2.p):1), type="n", 
     bty="n", xaxt="n", yaxt="n", xlab="", ylab="")
plot(SOM2.p[,"pA_estimate"], c(nrow(SOM2.p):1)-0.2, 
     xlim=c(0,0.5), yaxt="n",
     xlab="Detection probability", ylab="", pch=19, cex=0.5, 
     ylim=c(nrow(SOM2.p),1))
points(SOM2.p[,"pB_estimate"], c(nrow(SOM2.p):1), pch=17, cex=0.5)
points(SOM2.p[,"pC_estimate"], c(nrow(SOM2.p):1)+0.2, pch=15, cex=0.5)
segments(SOM2.p$pA_2.5..CI, c(nrow(SOM2.p):1)-0.2, 
         SOM2.p$pA_97.5..CI, c(nrow(SOM2.p):1)-0.2)
segments(SOM2.p$pB_2.5..CI, c(nrow(SOM2.p):1), 
         SOM2.p$pB_97.5..CI, c(nrow(SOM2.p):1))
segments(SOM2.p$pC_2.5..CI, c(nrow(SOM2.p):1)+0.2, 
         SOM2.p$pC_97.5..CI, c(nrow(SOM2.p):1)+0.2)
for(i in 1:nrow(SOM2.p)){
  lines(c(-0,1), c(i,i)-0.5, lty="dotted")
}
axis(2, at=c(nrow(SOM2.p):1), label=rownames(SOM2.p), las=1, cex.axis=0.9,
     tick=F)
plot(SOM2.fp[,"pA_estimate"], c(nrow(SOM2.p):1)-0.2, 
     xlim=c(0,0.5), yaxt="n",
     xlab="False positives", ylab="", pch=19, cex=0.5, 
     ylim=c(nrow(SOM2.p),1))
points(SOM2.fp[,"pB_estimate"], c(nrow(SOM2.fp):1), pch=17, cex=0.5)
points(SOM2.fp[,"pC_estimate"], c(nrow(SOM2.fp):1)+0.2, pch=15, cex=0.5)
segments(SOM2.fp$pA_2.5..CI, c(nrow(SOM2.fp):1)-0.2, 
         SOM2.fp$pA_97.5..CI, c(nrow(SOM2.fp):1)-0.2)
segments(SOM2.fp$pB_2.5..CI, c(nrow(SOM2.fp):1), 
         SOM2.fp$pB_97.5..CI, c(nrow(SOM2.fp):1))
segments(SOM2.fp$pC_2.5..CI, c(nrow(SOM2.fp):1)+0.2, 
         SOM2.fp$pC_97.5..CI, c(nrow(SOM2.fp):1)+0.2)
for(i in 1:nrow(SOM2.p)){
  lines(c(-0,1), c(i,i)-0.5, lty="dotted")
}


par(mar = c(4.5,1,2,1))
plot(POP_short$depth ~ POP_short$Pb.217.000,
     pch = 19, type = "o", lwd = 1.5,
     ylim = rev(range(POP_short$depth)),
     main = "Pb", xlab = "mg / kg", ylab = "",
     cex.main = 2, cex.axis = 1.5, cex.lab = 1.5,
     yaxt = "n")

plot(POP_short$depth ~ POP_short$DDE4.4,
     pch = 19, type = "o", lwd = 1.5,
     ylim = rev(range(POP_short$depth)),
     main = "DDE 4.4", xlab = "ug / kg", ylab = "",
     cex.main = 2, cex.axis = 1.5, cex.lab = 1.5,
     yaxt = "n")


# I will work with OTUs, so this may be not useful
# # Categories for sequence variants
# # Species abundance table from samples
# SamGregate = data.frame(name = EmblControlled$scientific_name,
#                         EmblControlled[,grep("sample.ST", names(EmblControlled))])
# SamCounts = aggregate(. ~ name, SamGregate, sum, na.action = na.exclude)
# rownames(SamCounts) = SamCounts$name
# SamCounts = SamCounts[,2:85]
# SamCounts = SamCounts[apply(SamCounts,1,sum) > 0,]

##### Methodology predictors of DNA data
OTUCounts = EmblControlled[,grep("sample.ST", names(EmblControlled))]

# Keep OTUs with at least some read
OTUCounts = OTUCounts[apply(OTUCounts,1,sum) > 0,]

# The data for this includes only the samples, but not the controls,
# as these have special diversiy and compositional contraints.

# Transpose the abundance matrix
OTUCountsT = t(OTUCounts)

# The same for experimental predictors
ExpPredictor = ExpSet[grep("ST01", rownames(ExpSet)),]

# Check sample correspondence
data.frame(rownames(OTUCountsT), rownames(ExpPredictor))

# PCNM vectors of the plate coordinates
PlatePCNM = pcnm(dist(ExpPredictor$column, ExpPredictor$row, method="euc"))

# PCNM visualizations
par(mfrow = c(2,4), mar = c(4,4,3,1))
for (i in colnames(scores(PlatePCNM))){
  ordisurf(ExpPredictor[,c(7,8)], scores(PlatePCNM, choi=i), 
           bubble = 4, main = i)
}

# Add the PCNM vectors to the experimental predictors
ExpPredictor = data.frame(ExpPredictor, scores(PlatePCNM))

# Forward selection infos from here: http://www.statmethods.net/stats/regression.html
## 1. DNA concentrations
ConcModel <- regsubsets(conc ~ kit + person + extract_order + weight,
                        data=ExpPredictor, nbest=10)

# Plot the model r2-s
par(mfrow = c(1,2))
plot(ConcModel,scale="r2")
subsets(ConcModel, statistic="rsq")

# R2 of best model
ConcModel.summary = summary(ConcModel)
max(ConcModel.summary$rsq)
ConcModel.summary$outmat[ConcModel.summary$rsq == max(ConcModel.summary$rsq),]

# Test the final model
conc.lm1 = lm(conc ~ kit + person + extract_order + weight,
              data=ExpPredictor[5:96,])

conc.lm2 = lm(conc ~ kit + extract_order + person + weight,
              data=ExpPredictor[5:96,])

# ANOVA tables
summary(conc.lm1)
kable(anova(conc.lm1))
kable(anova(conc.lm2))

# Plots
par(mar=c(4,4,1,1))

# Personnel differences
boxplot(conc ~ person, data=ExpPredictor[5:96,], 
        ylab = "DNA concentrations (ng/ul)", col="grey", boxwex=.5, notch=T)

# Kit differences
boxplot(conc ~ kit, data=ExpPredictor[5:96,], 
        ylab = "DNA concentrations (ng/ul)", col="grey", boxwex=.5, notch=T)

# Weight differences
plot(effect("weight", conc.lm1, multiline=T))

# Extraction order differences
plot(effect("extract_order", conc.lm1, multiline=T))

# plot(ExpPredictor[,"weight"], ExpPredictor[,"conc"], pch = 19,
#      xlab = "Sediment weight (mg)", ylab = "DNA concentration (ng/ul)")
# 
# abline(lm(conc ~ weight, data = ExpPredictor[5:96,]), lwd = 2)
# 
# plot(ExpPredictor[5:96,"extract_order"], ExpPredictor[5:96,"conc"], pch = 19,
#      xlab = "Extraction order", ylab = "DNA concentration (ng/ul)")
# abline(lm(conc ~ extract_order, data = ExpPredictor[5:96,]), lwd = 2)

## 2. Taxon richness and diversity

# Calculate Hill's series
CountsHill = renyi(OTUCountsT, hill = T)

# 2.1. Models of Hill's H0
H0.Model <- regsubsets(CountsHill$`0` ~ reads + kit + person + extract_order + 
                         pcr_order + weight + conc +
                         PCNM1 + PCNM2 + PCNM3 + PCNM4 + 
                         PCNM5 + PCNM6 + PCNM7,
                       data=ExpPredictor, nbest=10)

# Plot the model r2-s
summary.H0Model = summary(H0.Model)
plot(H0.Model,scale="r2")
subsets(H0.Model, statistic="rsq")

# Model with highest R2
summary.H0Model$outmat[summary.H0Model$rsq == max(summary.H0Model$rsq),]

# This model is way too complex, and many other models have similar rsq.
# I take the model with only 4 predictors that has rsq > 0.65
H0.compare = data.frame(subset = rownames(summary.H0Model$which), summary.H0Model$rsq, summary.H0Model$which)
# reads + kit + conc + PCNM3 + PCNM4

# # Similarly good models
# summary.H0Model$outmat[summary.H0Model$rsq > max(summary.H0Model$rsq) - 0.02,]

# Selected model:
H0.lm1 = lm(CountsHill$`0` ~ reads + kit + conc + PCNM3 + PCNM4, 
            data=ExpPredictor)

# H0 model statistics
summary(H0.lm1)
anova(H0.lm1)
kable(anova(H0.lm1))

# Plots of effects
plot(allEffects(H0.lm1))

# PCNM and richness
par(mar = c(2,2,1,1), mfrow = c(1,1))
plot(ExpPredictor$column, ExpPredictor$row, cex=CountsHill$`0`/150, pch=19)
ordisurf(ExpPredictor[,c(7,8)], scores(PlatePCNM, choi="PCNM4"), add = T)

coef(H0.lm1)
confint(H0.lm1)

# 2.2. Models of Hill's H1
H1.Model <- regsubsets(CountsHill$`1` ~ reads + kit + person + extract_order + 
                         pcr_order + weight + conc +
                         PCNM1 + PCNM2 + PCNM3 + PCNM4 + 
                         PCNM5 + PCNM6 + PCNM7,
                       data=ExpPredictor, nbest=10)

# Plot the model r2-s
par(mfrow = c(1,1), mar = c(4,4,1,1))
summary.H1Model = summary(H1.Model)
# plot(H1.Model,scale="r2")
subsets(H1.Model, statistic="rsq")

# Model with highest R2
summary.H1Model$outmat[summary.H1Model$rsq == max(summary.H1Model$rsq),]

# I take the model with only 4 predictors. rsq = 0.35877466
H1.compare = data.frame(subset = rownames(summary.H1Model$which), summary.H1Model$rsq, summary.H1Model$which)
# reads + person + pcr_order + weight + PCNM2

# Final model
H1.lm1 = lm(CountsHill$`1` ~ reads + person + pcr_order + weight + PCNM2, data=ExpPredictor)

# H2 model statistics
summary(H1.lm1)
anova(H1.lm1)
kable(anova(H1.lm1))

# Plots of effects
plot(allEffects(H1.lm1))

# plot(effect("pcr_order", H1.lm1, multiline=TRUE), ylab = "Hill's H1")

# 2.3. Models of Hill's H2
H2.Model <- regsubsets(CountsHill$`2` ~ reads + kit + person + extract_order + 
                         pcr_order + weight + conc +
                         PCNM1 + PCNM2 + PCNM3 + PCNM4 + 
                         PCNM5 + PCNM6 + PCNM7,
                       data=ExpPredictor, nbest=10)

# Plot the model r2-s
# par(mfrow = c(1,2), mar = c(4,4,1,1))
summary.H2Model = summary(H2.Model)
# plot(H2.Model,scale="r2")
subsets(H2.Model, statistic="rsq")

# Model with highest R2
# summary.H2Model$outmat[summary.H2Model$rsq == max(summary.H2Model$rsq),]

# Model with few variables but high Rsq Rsq = 
H2.compare = data.frame(subset = rownames(summary.H2Model$which), summary.H2Model$rsq, summary.H2Model$which)
# kit + person + extract_order + weight + conc

H2.lm1 = lm(CountsHill$`2` ~ kit + person + extract_order + weight + conc, data=ExpPredictor)

# H2 model statistics
summary(H2.lm1)
anova(H2.lm1)
kable(anova(H2.lm1))

# Plots of effects
# plot(effect("extract_order", H2.lm1, multiline=TRUE), ylab = "Hill's H2")
plot(allEffects(H2.lm1))

## 3. Community composition

### 3. Define core OTUs
## Summarize reads
TotCount = apply(OTUCountsT,2,sum)

## The average read number of OTUs
MeanCount=apply(OTUCountsT,2,function(vec) mean(vec[vec>0]))

## In how many samples is an OTU present?
TotPresent = apply(OTUCountsT,2,function(vec) sum(vec>0))

## The highest read number of an OTU in a sample
MaxCount=apply(OTUCountsT,2,max)

## Plotting incidence against abundance
plot(TotPresent, MaxCount, xlab="Incidence",
     ylab="Maximum Abundance", pch=20)

plot(TotPresent, log(MaxCount), xlab="Incidence",
     ylab="log(Maximum Abundance)", pch=20)

## Create a smoothed trendline
gam1 = gam(log(MaxCount)~s(TotPresent))

plot(gam1, residuals=T, shade=T, rug=F, cex=2.6,
     xlab="Incidence", ylab="logMean Abundance") # , xaxp=c(0,150,15)

## consider OTUs as core if present in at least 20 samples
IsFreq = TotPresent >= 20
OTU.some = OTUCountsT[,IsFreq]

core.mvabund = mvabund(OTUCountsT)

# Manual stepwise model selection. Predictor order established with
# assumptions about potential laboratory error importance, or the
# experiences from the previous tests. Predictors considered: only those that were at least 
# once significant in the previous models
# reads + kit + person + extract_order + weight + conc + PCNM4

# 3.1. Stepwise commmunity model selection

core.m1 = manyglm(core.mvabund ~ reads + kit + person + extract_order + weight + conc + PCNM4, 
                  data = ExpPredictor, family = "negative.binomial")

core.m2 = manyglm(core.mvabund ~ reads + kit + person + extract_order + weight + conc, 
                  data = ExpPredictor, family = "negative.binomial")

anova(core.m1, core.m2, nBoot = 10)

core.m3 = manyglm(core.mvabund ~ reads + kit + person + extract_order + weight, 
                  data = ExpPredictor, family = "negative.binomial")

anova(core.m2, core.m3, nBoot = 10)

core.m4 = manyglm(core.mvabund ~ reads + kit + person + extract_order, 
                  data = ExpPredictor, family = "negative.binomial")

anova(core.m3, core.m4, nBoot = 10)

# core.m5 = manyglm(core.mvabund ~ reads + kit + person, 
#                   data = ExpPredictor, family = "negative.binomial")
# 
# anova(core.m4, core.m5, nBoot = 10)
# 
# core.m6 = manyglm(core.mvabund ~ reads + kit + extract_order, 
#                   data = ExpPredictor, family = "negative.binomial")
# 
# anova(core.m4, core.m6, nBoot = 10)
# 
# core.m7 = manyglm(core.mvabund ~ reads + person + extract_order, 
#                   data = ExpPredictor, family = "negative.binomial")
# 
# anova(core.m4, core.m7, nBoot = 10)
# 
# core.m8 = manyglm(core.mvabund ~ person + extract_order, 
#                   data = ExpPredictor, family = "negative.binomial")
# 
# anova(core.m4, core.m8, nBoot = 10)

# A promising community model has reads + kit + person + extract_order + weight
m3.anova = anova(core.m3, nBoot = 300, p.uni = "adjusted")
m3.anova$table

# ## OTUs significantly explained by the affected by the experiment (at p<=0.01)
# p.ind.anova.mc <- as.data.frame(m3.anova$uni.p)
# colnames(p.ind.anova.mc)[p.ind.anova.mc[6,]<=0.01]

# 
# 
# methods.manyglm2 = manyglm(Samples.mvabund ~ reads + kit, 
#                            data = ExpPredictor, family = "negative.binomial")
# 
# methods.manyglm1$AICsum
# methods.manyglm2$AICsum
# 
# methods.manyglm3 = manyglm(Samples.mvabund ~ reads + person, 
#                            data = ExpPredictor, family = "negative.binomial")
# methods.manyglm3$AICsum
# 
# methods.manyglm4 = manyglm(Samples.mvabund ~ reads + person + extract_order, 
#                            data = ExpPredictor, family = "negative.binomial")
# methods.manyglm4$AICsum
# 
# methods.manyglm5 = manyglm(Samples.mvabund ~ reads + person + extract_order +
#                              pcr_order, 
#                            data = ExpPredictor, family = "negative.binomial")
# methods.manyglm5$AICsum
# 
# methods.manyglm6 = manyglm(Samples.mvabund ~ reads + person + extract_order +
#                              weight, 
#                            data = ExpPredictor, family = "negative.binomial")
# methods.manyglm6$AICsum
# 
# methods.manyglm7 = manyglm(Samples.mvabund ~ reads + person + extract_order +
#                              weight + conc, 
#                            data = ExpPredictor, family = "negative.binomial")
# methods.manyglm7$AICsum
# 
# methods.manyglm8 = manyglm(Samples.mvabund ~ reads + person + extract_order +
#                              weight + conc + PCNM1, 
#                            data = ExpPredictor, family = "negative.binomial")
# methods.manyglm8$AICsum
# 
# methods.manyglm9 = manyglm(Samples.mvabund ~ reads + person + extract_order +
#                              weight + conc + PCNM2, 
#                            data = ExpPredictor, family = "negative.binomial")
# methods.manyglm9$AICsum
# 
# methods.manyglm10 = manyglm(Samples.mvabund ~ reads + person + extract_order +
#                              weight + conc + PCNM3, 
#                            data = ExpPredictor, family = "negative.binomial")
# methods.manyglm10$AICsum
# 
# methods.manyglm11 = manyglm(Samples.mvabund ~ reads + person + extract_order +
#                              weight + conc + PCNM4, 
#                            data = ExpPredictor, family = "negative.binomial")
# methods.manyglm11$AICsum
# 
# methods.manyglm12 = manyglm(Samples.mvabund ~ reads + person + extract_order +
#                               weight + conc + PCNM5, 
#                             data = ExpPredictor, family = "negative.binomial")
# methods.manyglm12$AICsum
# 
# methods.manyglm13 = manyglm(Samples.mvabund ~ reads + person + extract_order +
#                               weight + conc + PCNM6, 
#                             data = ExpPredictor, family = "negative.binomial")
# methods.manyglm13$AICsum
# 
# methods.manyglm14 = manyglm(Samples.mvabund ~ reads + person + extract_order +
#                               weight + conc + PCNM7, 
#                             data = ExpPredictor, family = "negative.binomial")
# methods.manyglm14$AICsum
# 
# # The best model is 
# methods.manyglm7$call
# methods.manyglm7$AICsum
# plot(methods.manyglm7, which=c(1:4))
# 
# # 3.2. Model test
# community.anova = anova(methods.manyglm7, nBoot = 100, p.uni = "adjusted")
# kable(community.anova$table)
# community.summary = summary(methods.manyglm7, nBoot = 100, test = "LR")
# kable(community.summary$coefficients)








# Methods BORAL
# model-based ordination
# Estimate overdispersion parameters
# mvabund input matrix
EmblControlled[,grep("sample", names(EmblHead))]


OTU.mvabund = mvabund(OTUCountsT)
theta.model = manyglm(OTU.mvabund ~ data = ExpPredictor, 
                      family = "negative.binomial")
hist(core.m3$theta)

# Set overdispersion prior
set.prior = list(type = c("normal","normal","normal","uniform"),
                 hypparams = c(100, 20, 100, 20))

# LV ordination done on all OTUs with relatively low overdispersion
comm.ord = boral(OTUCountsT[,core.m4$theta < 50],
                 family = "negative.binomial", 
                 prior.control = set.prior, num.lv = 2, n.burnin = 10, 
                 n.iteration = 100, n.thin = 1)

# The "core" OTUs. Faster, but same patterns
# comm.ord.some = boral(OTU.some[,core.m3$theta < 50],
#                       X = ExpPredictor$reads,
#                       family = "negative.binomial", 
#                       prior.control = set.prior, num.lv = 2, n.burnin = 10, 
#                       n.iteration = 100, n.thin = 1)

# Person effects on community composition
ordicomm = ordiplot(comm.ord.some$lv.median, choices = c(1,2), type = "none", cex =0.5,
                    display = "sites")
points(ordicomm,"sites", pch=20, col=as.numeric(ExpPredictor$person))
ordiellipse(ordicomm, ExpPredictor$person,cex=.5,
            draw="polygon", col="black",
            alpha=200,kind="se",conf=0.95,
            show.groups=(c("Miki")))
ordiellipse(ordicomm, ExpPredictor$person,cex=.5,
            draw="polygon", col="red",
            alpha=200,kind="se",conf=0.95,
            show.groups=(c("Orsi")))
```

# Plot the ordinations
```{r eval=FALSE}
# par(mfrow = c(2,2), mar = c(2,2,2,1))
# predictors = c("reads", "extract_order", "weight", "conc")
# for (i in predictors) {
#   ordicomm = ordiplot(comm.ord$lv.median, choices = c(1,2), type = "none", cex =0.5,
#                       display = "sites", xlim = c(-0.3,0.3))
#   points(ordicomm,"sites", pch=20, col=as.numeric(ExpPredictor$person))
#   ordisurf(ordicomm, ExpPredictor[,i], add=T, col = "black", main=i)
# }
```

Estimate overdispersion parameters for each OTU with `mvabund`.
Fit a negative binomial model where the single predictor is the number
of reads of each sample.
```{r eval=FALSE}
# mvabund input matrix
Repli.mvabund = mvabund(t(RepliMatrix))

# simple model, similar to the LVM ordination
theta.model = manyglm(Repli.mvabund ~ reads, data = RepliExp,
                      family = "negative.binomial")
```

```{r include=FALSE, eval=FALSE}
# diagnostics
plot(theta.model, which = c(1:3))
```

A few OTUs have weird dispersion parameters. 
```{r eval=FALSE}
# distribution of the overdispersion parameters
hist(theta.model$theta)
hist(log(theta.model$theta)) 
# rownames(RepliMatrix[theta.model$theta > 30,])
```

Model-based ordination with `boral`. 
Running the model with these parameters takes a very long time. 
Try lowering them for a quick(er) look, e.g.
n.burnin=10, n.iteration=100, n.thin=1.
```{r eval = FALSE}
# Set the overdispersion prior
# set.prior = list(type = c("normal","normal","normal","uniform"),
#                  hypparams = c(100, 20, 100, 20))
# 
# # LVM ordination on OTUs with relatively low overdispersion
# # This run would take a very long time.
# comm.ord = boral(t(RepliMatrix)[,theta.model$theta < 30],
#                  X = RepliExp$reads,
#                  family = "negative.binomial",
#                  prior.control = set.prior, num.lv = 2, n.burnin = 1000,
#                  n.iteration = 4000, n.thin = 3)
```

Plot the ordination. Currently just include a previous ordination. The `.Rdata` file with the complete ordination run should be loaded once ready
with `load.image()`, or knitted with `rmarkdown::render("your_doc.Rmd")`.
```{r eval=FALSE}
# Colors and symbols for samples
RepliExpColors = data.frame(RepliExp, 
                            symbols = c(c(rep(22,4), rep(23,2), 
                                          rep(24,4), rep(25,2)),
                                        na.omit(RepliExp$depth.nominal)))
sample_colors = c(levels(factor(RepliExpColors$symbols*10)))
sample_legend = c(levels(factor(RepliExpColors$depth)), 
                  c("Extraction control",
                    "Multiplex control",
                    "PCR control",
                    "Mock community"))

# The LVM ordination plot
par(mfrow = c(1,1), mar = c(4,4.5,1,1))
ordicomm = ordiplot(comm.ord$lv.median, choices = c(1,2), 
                    type = "none", cex =0.5,
                    display = "sites", xlab = "Community axis 1",
                    ylab = "Community axis 2", cex.lab = 1.5)
points(ordicomm, "sites", lwd=2, 
       col=RepliExpColors$symbols*10, 
       pch = RepliExpColors$symbols)
ordispider(ordicomm, RepliExpColors$symbols, 
           col=sample_colors, lwd = 2)
ordisurf(ordicomm, RepliExpColors$depth, add=T, col = "darkgrey")
legend(-1.8, 0.9, sample_legend, border="white", bty="n", lwd = 2,
col=sample_colors, pch = as.numeric(sample_colors)/10, cex = 0.5)
```

For now just load the plot from a previous short `boral` run.
![](boral_plot.png)
The replicates can be further evaluated, but this is not shown for now.
```{r include=FALSE, eval=FALSE}
# Further evaluations of samples from the ordination
# color by person
# points(ordicomm,"sites", lwd=2, 
#        col=as.numeric(RepliExp$person)*100, pch = RepliExp$symbols)
# legend(-1.8, 0.8, sample_legend, border="white", bty="n", lwd = 2,
# col=sample_colors, pch = as.numeric(sample_colors)/10, cex = 0.9)
# legend for person
# legend(-1.8, 0, levels(RepliExp$person), 
#        border="white", bty="n", lwd = 2,
#        col=as.numeric(as.factor(levels(RepliExp$person)))*100, 
#        pch = 19, cex = 0.9)
# legend(-1.1, 1.15, "colors - samples/controls, symbols - replicates of sample/control, contours - depth", 
# border="white", bty="n", cex = 0.9)
# text(ordicomm,"sites",rownames(RepliExp), cex=0.5)
# dev.off()

# # plot for technical replicate explanation
# ExplainColors = RepliExp$symbols
# ExplainColors[ExplainColors != 10] <- 0
# 
# ExplainSampleColors = c(levels(factor(RepliExp$symbols*10)))
# ExplainSampleColors[ExplainSampleColors != 100] <- 0
# 
# par(mfrow = c(1,1), mar = c(4,4,1,1))
# ordicomm = ordiplot(comm.ord$lv.median, choices = c(1,2), type = "none", cex =0.5,
#                     display = "sites")
# points(ordicomm, "sites", lwd=2, 
#        col=ExplainColors*10, pch = RepliExp$symbols)
# # color by person
# # points(ordicomm,"sites", lwd=2, 
# #        col=as.numeric(RepliExp$person)*100, pch = RepliExp$symbols)
# ordispider(ordicomm$sites, RepliExp$symbols, 
#            col=ExplainSampleColors, lwd = 2)
# ordisurf(ordicomm, RepliExp$depth, add=T, col = "grey")
# legend(-1.8, 0.8, sample_legend, border="white", bty="n", lwd = 2,
#        col=sample_colors, pch = as.numeric(sample_colors)/10, cex = 0.9)
# # legend for person
# # legend(-1.8, 0, levels(RepliExp$person), 
# #        border="white", bty="n", lwd = 2,
# #        col=as.numeric(as.factor(levels(RepliExp$person)))*100, 
# #        pch = 19, cex = 0.9)
# legend(-1.1, 1.15, "colors - samples/controls, symbols - replicates of sample/control, contours - depth", 
#        border="white", bty="n", cex = 0.9)
# # text(ordicomm,"sites",rownames(RepliExp), cex=0.5)

# Family names in the positive controls
# levels(factor(EmblHead$family_name[
#   apply(EmblHead[,grep("sample.POS",names(EmblHead))],1,sum) > 0]))
```

Which are the weird replicates? 
I will remove them once the negatives are cleared
and the low reads are controlled with the positives.
```{r}
RepliProblems = c("sample.ST01.10.rep.1.MN_S1",
                  "sample.ST01.7.5.rep.1.MB_S90",
                  "sample.ST01.9.rep.1.MB_S14",
                  "sample.ST01.5.5.rep.1.MN_S27",
                  "sample.ST01.3.5.rep.1.MB_S12",
                  "sample.ST01.5.5.rep.1.MB_S66",
                  "sample.ST01.3.rep.1.MB_S40",
                  "sample.ST01.2.rep.1.MB_S62",
                  "sample.ST01.1.rep.2.MN_S51",
                  "sample.ST01.8.5.rep.1.MB_S76")
```

### Plot all samples, replicates, controls.
Technical replicates should go together. Controls should be distant from the samples.
```{r}
# OTU abundance matrix
RepliMatrix = EmblHead[,grep('sample.', names(EmblHead))]
RepliMatrix = RepliMatrix[,1:96]

# Remove OTUs with no observations
RepliMatrix = RepliMatrix[apply(RepliMatrix,1,sum) > 0, ]
summary(apply(RepliMatrix, 1,sum))
```

### False positive control with detections in replicates
Remove OTUs if not observed in at least 2 out of 4 replicates of a horizon
How to do this was found here: http://stackoverflow.com/questions/9704213/r-remove-part-of-string

Get the sample names. 
```{r eval=FALSE}
SampleNames = levels(as.factor(sapply(strsplit(names(NoWeirdMatrix),
                                               split='ep', fixed=TRUE),
                                      function(x) (x[1]))))
```

In how many replicates observed per sample?
```{r eval=FALSE}
PresentReps = data.frame(row.names = rownames(NoWeirdMatrix))
for (i in SampleNames){
  ActualSet = grep(i, names(NoWeirdMatrix))
  Selected = NoWeirdMatrix[ActualSet]
  Selected[Selected > 0] <- 1 # set the read numbers to 1
  PresentReps = cbind(PresentReps, apply(Selected, 1, sum))
}
colnames(PresentReps) = SampleNames
```

```{r eval=FALSE}
TrialMatrix = NoWeirdMatrix

SelectSample = names(PresentReps)[21]

SelectMatrix = grep(SelectSample, names(TrialMatrix))

dim(TrialMatrix[,SelectMatrix])

FilterPresent = PresentReps[,SelectSample] < 2

TrialMatrix[FilterPresent,SelectMatrix] <- 0

```


### Remove weird replicates
Visually selected from the `boral` plot above
```{r eval=FALSE}
# Abundance matrix without the controls
IndexHorizon = grep("sample.ST", names(RareControlled))

# A filter for weird horizon replicates
RepliProblemFilter = 
  names(RareControlled)[IndexHorizon] %in% RepliProblems

# Only the samples kept
HorizonMatrix = RareControlled[, IndexHorizon]

# Weird replicates removed
NoWeirdMatrix = HorizonMatrix[, !(RepliProblemFilter)]
```




In low read samples only read numbers may drive OTU observations. Since samples were not multiplexed before PCR we assume that read numbers stand for PCR performance.
```{r eval=FALSE}
plot(specnumber(OTULab) ~ ExpLab$reads, 
     main = "Richness VS read numbers",
     xlab = "No. reads", ylab = "Richness",
     pch=19)
```

In samples with less than 3000 reads only the PCR efficiency drives richness. I remove those samples.

```{r eval=FALSE}
FilterSample = ExpLab$reads >= 3000

# Remove samples with less, than 3000 reads
OTULab = OTULab[FilterSample,]

# remove OTUs with no reads left
OTULab = OTULab[,apply(OTULab,2,sum)>0]
```

Adjust the lab predictors
```{r}
FilterLab = rownames(ExpLab) %in% rownames(OTULab)
ExpLab = ExpLab[FilterLab,]
```

### Explore some relationships.
Richness VS read numbers
```{r eval=FALSE}
plot(HillLab$hill1 ~ ExpLab$reads, 
     main = "Richness VS read numbers",
     xlab = "No. reads", ylab = "Richness",
     pch=19)
```

Richness by lab person
```{r eval=FALSE}
boxplot(HillLab$hill1 ~ ExpLab$person,
        main = "Richness by lab person",
        xlab = "Person",
        ylab = "Richness")
```

DNA concentration by lab person
```{r eval=FALSE}
boxplot(ExpLab$conc ~ ExpLab$person,
        main = "DNA concentration by lab person",
        xlab = "Person",
        ylab = "DNA concentration (ng/ul)")
```

Richness by DNA concentration
```{r eval=FALSE}
plot(HillLab$hill1 ~ ExpLab$conc, 
     main = "Richness by DNA concentration",
     xlab = "DNA concentration (ng/ul)", ylab = "Richness",
     pch=19)
```

Richness by extraction order
```{r eval=FALSE}
plot(HillLab$hill1 ~ ExpLab$extract_order, 
     main = "Richness by extraction order",
     xlab = "Extraction order", ylab = "Richness",
     pch=19)
```

Depth by extraction order
```{r eval=FALSE}
plot(ExpLab$depth ~ ExpLab$extract_order, 
     main = "Depth by extraction order",
     xlab = "Extraction order", ylab = "Depth (cm)",
     pch=19)
```

DNA concentration by extraction order
```{r eval=FALSE}
plot(ExpLab$conc ~ ExpLab$extract_order, 
     main = "DNA concentration by extraction order",
     xlab = "Extraction order", ylab = "DNA concentratio (ng/ul)",
     pch=19)
```

DNA concentration by kit (MB: MoBio, MN: Macherey-Nagel)
```{r eval=FALSE}
plot(ExpLab$conc ~ ExpLab$kit, 
     main = "DNA concentration by kit",
     xlab = "Kit", ylab = "DNA concentration (ng/ul)",
     pch=19)
```

Richness by kit (MB: MoBio, MN: Macherey-Nagel)
```{r eval=FALSE}
plot(HillLab$hill1 ~ ExpLab$kit, 
     main = "Richness by kit",
     xlab = "Kit", ylab = "Richness",
     pch=19)
```

Number of reads by PCR order
```{r eval=FALSE}
plot(ExpLab$reads ~ ExpLab$pcr_order, 
     main = "Number of reads by PCR order",
     xlab = "PCR order", ylab = "Richness",
     pch=19)
```

DNA concentration by sample weight
```{r eval=FALSE}
plot(ExpLab$conc ~ ExpLab$weight, 
     main = "DNA concentration by sample weight",
     xlab = "Sample weight (g)", ylab = "DNA concentration (ng/ul)",
     pch=19)
```

##### Explore the read abundances
```{r eval=FALSE}
pdf(file="OTU_plots.pdf", height = 100, width = 7)
par(mfrow=c(68,5), mar = c(3,2,2,0.5))
for(i in names(OTULab)){
  plot(ExpLab$age, OTULab[,i], pch=19, cex = 0.5,
       xlab = "age", ylab="", main = i,
       col = (as.numeric(ExpLab$nuclear)+10)^2)
}
dev.off()
```

```{r eval=FALSE}
plot(OTULab[,"M01271:3:000000000-D0YHF:1:1101:26980:13987_CONS_SUB"])
```

Model coefficients for nuclear, confidence intervals
```{r eval=FALSE}
CoefDuring = 
  OTU.horizon.read.kit.order.nucl$coefficients[25,]
StdErrDuring = 
  OTU.horizon.read.kit.order.nucl$stderr.coefficients[25,]

CI.2.5.During = CoefDuring-2*StdErrDuring
CI.97.5.During = CoefDuring+2*StdErrDuring

CoefAfter = 
  OTU.horizon.read.kit.order.nucl$coefficients[26,]
StdErrAfter = 
  OTU.horizon.read.kit.order.nucl$stderr.coefficients[26,]

CI.2.5.After = CoefAfter-2*StdErrAfter
CI.97.5.After = CoefAfter+2*StdErrAfter


c(mean(x)-2*sem,mean(x)+2*sem)

Coefs = data.frame(CoefDuring,CoefAfter,
                   StdErrDuring,StdErrAfter,
                   CI.2.5.During, CI.97.5.During,
                   CI.2.5.After, CI.97.5.After)
```

Filter consistent coefficients
```{r eval=FALSE}
FilterDuring = 
  c(Coefs$CI.2.5.During > 0 & Coefs$CI.97.5.During > 0) |
  c(Coefs$CI.2.5.During < 0 & Coefs$CI.97.5.During < 0)

FilterAfter = 
  c(Coefs$CI.2.5.After > 0 & Coefs$CI.97.5.After > 0) |
  c(Coefs$CI.2.5.After < 0 & Coefs$CI.97.5.After < 0)

seq(0,max(OTULab[,rownames(Coefs[FilterDuring,])]))

plot(ExpLab$age,
     seq(0,max(OTULab[,rownames(Coefs[FilterDuring,])]),
         length.out = 76),
     type="n")
for (i in rownames(Coefs[FilterDuring,])){
  color = 1
  points(ExpLab$age, OTULab[,i], pch=19)
  # color = color + 1
}


rownames(Coefs[FilterDuring,])
```


Are there some coefficients were the std. err. is not exceedingly high (e.g. less, than 10X of the coefficient)?
```{r eval=FALSE}
FilterDuring = 
  10*abs(Coefs$CoefDuring) >= Coefs$StdErrDuring
FilterAfter = 
  10*abs(Coefs$CoefAfter) >= Coefs$StdErrAfter

rownames(Coefs[FilterAfter,]) %in% rownames(Coefs[FilterDuring,])

# Coefficients maybe interesting
CoefPlot = 
  Coefs[FilterDuring | FilterAfter,]
```

plot coefficients
```{r eval=FALSE}
plot(min(CoefPlot):max(CoefPlot),
     c(1:nrow(CoefPlot)))
```


```{r eval=FALSE}
# OTU.summary = summary(OTU.horizon.read.kit.order.nucl,
#                       nBoot = 10, test = "LR")
```

```{r eval=FALSE}
# OTU.summary$coefficients
```